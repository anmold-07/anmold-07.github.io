<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Digital Notebook</title><link href="https://anmold-07.github.io/" rel="alternate"></link><link href="https://anmold-07.github.io/feeds/all.atom.xml" rel="self"></link><id>https://anmold-07.github.io/</id><updated>2020-12-01T19:30:00-05:00</updated><entry><title>References for Probabilistic Graphical Models</title><link href="https://anmold-07.github.io/2020/09/01/my-super-post/" rel="alternate"></link><published>2020-09-01T10:25:00-04:00</published><updated>2020-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-09-01:/2020/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Probabilistic Graphical Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;This course has many references on the internet. A lot also depends on what aspect of the course one would want to focus on and the motivation behind taking the course. If you are new to PGMs and have a decent background in probability theory, then I would highly recommend that you go through &lt;a href="http://web.mit.edu/6.008/www/videos/index.html"&gt;these&lt;/a&gt; videos first. These lectures also are part of the Computational Probability and Inference course on edX. It builds the right motivation for PGMs and it was helpful to go through a few videos before officially diving deep in PGMs.&lt;/p&gt;
&lt;p&gt;Nevertheless, from a research point of view, I found the following references very helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/dsontag/courses/pgm13/"&gt;This&lt;/a&gt; course taught by Dr. Sontag provides a rigorous holistic overview of PGMs.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cse.iitd.ac.in/~parags/teaching/2015/au15/col776/"&gt;This&lt;/a&gt; course taught by Dr. Singla provides a good perspective on learning PGMs. It also has many additional references that are very helpful for applying the theory to many research problems.&lt;/p&gt;
&lt;p&gt;One of the most comprehensive reference for PGMs are &lt;a href="https://sailinglab.github.io/pgm-spring-2019/"&gt;this&lt;/a&gt; and &lt;a href="https://www.zabaras.com/probabilistic-graphical-models"&gt;this&lt;/a&gt; course taught by the Dr. Eric P. Xing and Dr. Zabaras respectively. They cover a lot of material in a short span of time so it's important that you have some prior knowledge of machine learning in general. Each topic is covered in extreme detail and the video lectures/slides help understand the material better. It brings all of Machine learning together that helps builds the right perspective in my opinion. These long videos are worth the time.&lt;/p&gt;
&lt;p&gt;These &lt;a href="https://ermongroup.github.io/cs228-notes/"&gt;course notes&lt;/a&gt; by Stanford University are also a very handy reference. Finally, &lt;a href="https://www.coursera.org/specializations/probabilistic-graphical-models"&gt;this&lt;/a&gt; COURSERA specialization is also very helpful and serves to provide sound theoretical knowledge and also gives the opportunity to apply the inference and learning algorithms to various practical problems. Highly recommend this course.  &lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="machine learning"></category><category term="graph theory"></category><category term="learning"></category><category term="inference"></category><category term="factorization"></category><category term="modeling"></category><category term="markov networks"></category><category term="bayesian networks"></category><category term="posterior probability"></category></entry><entry><title>References for Probabilistic Graphical Models</title><link href="https://anmold-07.github.io/2020/09/01/my-super-post/" rel="alternate"></link><published>2020-09-01T10:25:00-04:00</published><updated>2020-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-09-01:/2020/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Probabilistic Graphical Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;This course has many references on the internet. A lot also depends on what aspect of the course one would want to focus on and the motivation behind taking the course. If you are new to PGMs and have a decent background in probability theory, then I would highly recommend that you go through &lt;a href="http://web.mit.edu/6.008/www/videos/index.html"&gt;these&lt;/a&gt; videos first. These lectures also are part of the Computational Probability and Inference course on edX. It builds the right motivation for PGMs and it was helpful to go through a few videos before officially diving deep in PGMs.&lt;/p&gt;
&lt;p&gt;Nevertheless, from a research point of view, I found the following references very helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/dsontag/courses/pgm13/"&gt;This&lt;/a&gt; course taught by Dr. Sontag provides a rigorous holistic overview of PGMs.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cse.iitd.ac.in/~parags/teaching/2015/au15/col776/"&gt;This&lt;/a&gt; course taught by Dr. Singla provides a good perspective on learning PGMs. It also has many additional references that are very helpful for applying the theory to many research problems.&lt;/p&gt;
&lt;p&gt;One of the most comprehensive reference for PGMs are &lt;a href="https://sailinglab.github.io/pgm-spring-2019/"&gt;this&lt;/a&gt; and &lt;a href="https://www.zabaras.com/probabilistic-graphical-models"&gt;this&lt;/a&gt; course taught by the Dr. Eric P. Xing and Dr. Zabaras respectively. They cover a lot of material in a short span of time so it's important that you have some prior knowledge of machine learning in general. Each topic is covered in extreme detail and the video lectures/slides help understand the material better. It brings all of Machine learning together that helps builds the right perspective in my opinion. These long videos are worth the time.&lt;/p&gt;
&lt;p&gt;These &lt;a href="https://ermongroup.github.io/cs228-notes/"&gt;course notes&lt;/a&gt; by Stanford University are also a very handy reference. Finally, &lt;a href="https://www.coursera.org/specializations/probabilistic-graphical-models"&gt;this&lt;/a&gt; COURSERA specialization is also very helpful and serves to provide sound theoretical knowledge and also gives the opportunity to apply the inference and learning algorithms to various practical problems. Highly recommend this course.  &lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="machine learning"></category><category term="graph theory"></category><category term="learning"></category><category term="inference"></category><category term="factorization"></category><category term="modeling"></category><category term="markov networks"></category><category term="bayesian networks"></category><category term="posterior probability"></category></entry><entry><title>References for Probabilistic Graphical Models</title><link href="https://anmold-07.github.io/2020/09/01/my-super-post/" rel="alternate"></link><published>2020-09-01T10:25:00-04:00</published><updated>2020-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-09-01:/2020/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Probabilistic Graphical Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;This course has many references on the internet. A lot also depends on what aspect of the course one would want to focus on and the motivation behind taking the course. If you are new to PGMs and have a decent background in probability theory, then I would highly recommend that you go through &lt;a href="http://web.mit.edu/6.008/www/videos/index.html"&gt;these&lt;/a&gt; videos first. These lectures also are part of the Computational Probability and Inference course on edX. It builds the right motivation for PGMs and it was helpful to go through a few videos before officially diving deep in PGMs.&lt;/p&gt;
&lt;p&gt;Nevertheless, from a research point of view, I found the following references very helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/dsontag/courses/pgm13/"&gt;This&lt;/a&gt; course taught by Dr. Sontag provides a rigorous holistic overview of PGMs.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cse.iitd.ac.in/~parags/teaching/2015/au15/col776/"&gt;This&lt;/a&gt; course taught by Dr. Singla provides a good perspective on learning PGMs. It also has many additional references that are very helpful for applying the theory to many research problems.&lt;/p&gt;
&lt;p&gt;One of the most comprehensive reference for PGMs are &lt;a href="https://sailinglab.github.io/pgm-spring-2019/"&gt;this&lt;/a&gt; and &lt;a href="https://www.zabaras.com/probabilistic-graphical-models"&gt;this&lt;/a&gt; course taught by the Dr. Eric P. Xing and Dr. Zabaras respectively. They cover a lot of material in a short span of time so it's important that you have some prior knowledge of machine learning in general. Each topic is covered in extreme detail and the video lectures/slides help understand the material better. It brings all of Machine learning together that helps builds the right perspective in my opinion. These long videos are worth the time.&lt;/p&gt;
&lt;p&gt;These &lt;a href="https://ermongroup.github.io/cs228-notes/"&gt;course notes&lt;/a&gt; by Stanford University are also a very handy reference. Finally, &lt;a href="https://www.coursera.org/specializations/probabilistic-graphical-models"&gt;this&lt;/a&gt; COURSERA specialization is also very helpful and serves to provide sound theoretical knowledge and also gives the opportunity to apply the inference and learning algorithms to various practical problems. Highly recommend this course.  &lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="machine learning"></category><category term="graph theory"></category><category term="learning"></category><category term="inference"></category><category term="factorization"></category><category term="modeling"></category><category term="markov networks"></category><category term="bayesian networks"></category><category term="posterior probability"></category></entry><entry><title>References for Probabilistic Graphical Models</title><link href="https://anmold-07.github.io/2020/09/01/my-super-post/" rel="alternate"></link><published>2020-09-01T10:25:00-04:00</published><updated>2020-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-09-01:/2020/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Probabilistic Graphical Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;This course has many references on the internet. A lot also depends on what aspect of the course one would want to focus on and the motivation behind taking the course. If you are new to PGMs and have a decent background in probability theory, then I would highly recommend that you go through &lt;a href="http://web.mit.edu/6.008/www/videos/index.html"&gt;these&lt;/a&gt; videos first. These lectures also are part of the Computational Probability and Inference course on edX. It builds the right motivation for PGMs and it was helpful to go through a few videos before officially diving deep in PGMs.&lt;/p&gt;
&lt;p&gt;Nevertheless, from a research point of view, I found the following references very helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/dsontag/courses/pgm13/"&gt;This&lt;/a&gt; course taught by Dr. Sontag provides a rigorous holistic overview of PGMs.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cse.iitd.ac.in/~parags/teaching/2015/au15/col776/"&gt;This&lt;/a&gt; course taught by Dr. Singla provides a good perspective on learning PGMs. It also has many additional references that are very helpful for applying the theory to many research problems.&lt;/p&gt;
&lt;p&gt;One of the most comprehensive reference for PGMs are &lt;a href="https://sailinglab.github.io/pgm-spring-2019/"&gt;this&lt;/a&gt; and &lt;a href="https://www.zabaras.com/probabilistic-graphical-models"&gt;this&lt;/a&gt; course taught by the Dr. Eric P. Xing and Dr. Zabaras respectively. They cover a lot of material in a short span of time so it's important that you have some prior knowledge of machine learning in general. Each topic is covered in extreme detail and the video lectures/slides help understand the material better. It brings all of Machine learning together that helps builds the right perspective in my opinion. These long videos are worth the time.&lt;/p&gt;
&lt;p&gt;These &lt;a href="https://ermongroup.github.io/cs228-notes/"&gt;course notes&lt;/a&gt; by Stanford University are also a very handy reference. Finally, &lt;a href="https://www.coursera.org/specializations/probabilistic-graphical-models"&gt;this&lt;/a&gt; COURSERA specialization is also very helpful and serves to provide sound theoretical knowledge and also gives the opportunity to apply the inference and learning algorithms to various practical problems. Highly recommend this course.  &lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="machine learning"></category><category term="graph theory"></category><category term="learning"></category><category term="inference"></category><category term="factorization"></category><category term="modeling"></category><category term="markov networks"></category><category term="bayesian networks"></category><category term="posterior probability"></category></entry><entry><title>References for Probabilistic Graphical Models</title><link href="https://anmold-07.github.io/2020/09/01/my-super-post/" rel="alternate"></link><published>2020-09-01T10:25:00-04:00</published><updated>2020-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-09-01:/2020/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Probabilistic Graphical Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;This course has many references on the internet. A lot also depends on what aspect of the course one would want to focus on and the motivation behind taking the course. If you are new to PGMs and have a decent background in probability theory, then I would highly recommend that you go through &lt;a href="http://web.mit.edu/6.008/www/videos/index.html"&gt;these&lt;/a&gt; videos first. These lectures also are part of the Computational Probability and Inference course on edX. It builds the right motivation for PGMs and it was helpful to go through a few videos before officially diving deep in PGMs.&lt;/p&gt;
&lt;p&gt;Nevertheless, from a research point of view, I found the following references very helpful:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/dsontag/courses/pgm13/"&gt;This&lt;/a&gt; course taught by Dr. Sontag provides a rigorous holistic overview of PGMs.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cse.iitd.ac.in/~parags/teaching/2015/au15/col776/"&gt;This&lt;/a&gt; course taught by Dr. Singla provides a good perspective on learning PGMs. It also has many additional references that are very helpful for applying the theory to many research problems.&lt;/p&gt;
&lt;p&gt;One of the most comprehensive reference for PGMs are &lt;a href="https://sailinglab.github.io/pgm-spring-2019/"&gt;this&lt;/a&gt; and &lt;a href="https://www.zabaras.com/probabilistic-graphical-models"&gt;this&lt;/a&gt; course taught by the Dr. Eric P. Xing and Dr. Zabaras respectively. They cover a lot of material in a short span of time so it's important that you have some prior knowledge of machine learning in general. Each topic is covered in extreme detail and the video lectures/slides help understand the material better. It brings all of Machine learning together that helps builds the right perspective in my opinion. These long videos are worth the time.&lt;/p&gt;
&lt;p&gt;These &lt;a href="https://ermongroup.github.io/cs228-notes/"&gt;course notes&lt;/a&gt; by Stanford University are also a very handy reference. Finally, &lt;a href="https://www.coursera.org/specializations/probabilistic-graphical-models"&gt;this&lt;/a&gt; COURSERA specialization is also very helpful and serves to provide sound theoretical knowledge and also gives the opportunity to apply the inference and learning algorithms to various practical problems. Highly recommend this course.  &lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="machine learning"></category><category term="graph theory"></category><category term="learning"></category><category term="inference"></category><category term="factorization"></category><category term="modeling"></category><category term="markov networks"></category><category term="bayesian networks"></category><category term="posterior probability"></category></entry><entry><title>References for Statistical Signal Processing</title><link href="https://anmold-07.github.io/2020/01/01/my-super-post/" rel="alternate"></link><published>2020-01-01T10:25:00-05:00</published><updated>2020-05-01T19:30:00-04:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-01-01:/2020/01/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Statistical Signal Processing&lt;/p&gt;</summary><content type="html">&lt;p&gt;The topics covered in a course on Statistical Signal Processing and Estimation and Detection Theory are more or less similar. The following is a breakdown.
A course on estimation theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sufficient statistics&lt;/li&gt;
&lt;li&gt;Minimum variance unbiased estimation&lt;/li&gt;
&lt;li&gt;Cramer-Rao lower bound&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Bayesian estimation&lt;/li&gt;
&lt;li&gt;Wiener and Kalman filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A course on detection theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian risk theory&lt;/li&gt;
&lt;li&gt;Neyman-Pearson detection&lt;/li&gt;
&lt;li&gt;Signal detection in Gaussian noise&lt;/li&gt;
&lt;li&gt;Bayes factors and GLRTs&lt;/li&gt;
&lt;li&gt;CFAR detection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I found the following references helpful:&lt;/p&gt;
&lt;p&gt;Course notes for &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-432-stochastic-processes-detection-and-estimation-spring-2004/index.htm"&gt;this course&lt;/a&gt; was very helpful. 
To find the course notes refer to this &lt;a href="https://www.rle.mit.edu/sia/wp-content/uploads/2015/04/chapter1.pdf"&gt;link.&lt;/a&gt; and iterate through the chapters through links. I found these set of notes to be very comprehensive and nicely written.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/"&gt;This&lt;/a&gt; is another excellent reference with video lectures available on YouTube.
&lt;a href="https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/"&gt;This&lt;/a&gt; reference also covers a broad range of topic in depth.&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="detection theory"></category><category term="estimation theory"></category><category term="signal processing"></category></entry><entry><title>References for Statistical Signal Processing</title><link href="https://anmold-07.github.io/2020/01/01/my-super-post/" rel="alternate"></link><published>2020-01-01T10:25:00-05:00</published><updated>2020-05-01T19:30:00-04:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-01-01:/2020/01/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Statistical Signal Processing&lt;/p&gt;</summary><content type="html">&lt;p&gt;The topics covered in a course on Statistical Signal Processing and Estimation and Detection Theory are more or less similar. The following is a breakdown.
A course on estimation theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sufficient statistics&lt;/li&gt;
&lt;li&gt;Minimum variance unbiased estimation&lt;/li&gt;
&lt;li&gt;Cramer-Rao lower bound&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Bayesian estimation&lt;/li&gt;
&lt;li&gt;Wiener and Kalman filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A course on detection theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian risk theory&lt;/li&gt;
&lt;li&gt;Neyman-Pearson detection&lt;/li&gt;
&lt;li&gt;Signal detection in Gaussian noise&lt;/li&gt;
&lt;li&gt;Bayes factors and GLRTs&lt;/li&gt;
&lt;li&gt;CFAR detection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I found the following references helpful:&lt;/p&gt;
&lt;p&gt;Course notes for &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-432-stochastic-processes-detection-and-estimation-spring-2004/index.htm"&gt;this course&lt;/a&gt; was very helpful. 
To find the course notes refer to this &lt;a href="https://www.rle.mit.edu/sia/wp-content/uploads/2015/04/chapter1.pdf"&gt;link.&lt;/a&gt; and iterate through the chapters through links. I found these set of notes to be very comprehensive and nicely written.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/"&gt;This&lt;/a&gt; is another excellent reference with video lectures available on YouTube.
&lt;a href="https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/"&gt;This&lt;/a&gt; reference also covers a broad range of topic in depth.&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="detection theory"></category><category term="estimation theory"></category><category term="signal processing"></category></entry><entry><title>References for Statistical Signal Processing</title><link href="https://anmold-07.github.io/2020/01/01/my-super-post/" rel="alternate"></link><published>2020-01-01T10:25:00-05:00</published><updated>2020-05-01T19:30:00-04:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-01-01:/2020/01/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Statistical Signal Processing&lt;/p&gt;</summary><content type="html">&lt;p&gt;The topics covered in a course on Statistical Signal Processing and Estimation and Detection Theory are more or less similar. The following is a breakdown.
A course on estimation theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sufficient statistics&lt;/li&gt;
&lt;li&gt;Minimum variance unbiased estimation&lt;/li&gt;
&lt;li&gt;Cramer-Rao lower bound&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Bayesian estimation&lt;/li&gt;
&lt;li&gt;Wiener and Kalman filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A course on detection theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian risk theory&lt;/li&gt;
&lt;li&gt;Neyman-Pearson detection&lt;/li&gt;
&lt;li&gt;Signal detection in Gaussian noise&lt;/li&gt;
&lt;li&gt;Bayes factors and GLRTs&lt;/li&gt;
&lt;li&gt;CFAR detection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I found the following references helpful:&lt;/p&gt;
&lt;p&gt;Course notes for &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-432-stochastic-processes-detection-and-estimation-spring-2004/index.htm"&gt;this course&lt;/a&gt; was very helpful. 
To find the course notes refer to this &lt;a href="https://www.rle.mit.edu/sia/wp-content/uploads/2015/04/chapter1.pdf"&gt;link.&lt;/a&gt; and iterate through the chapters through links. I found these set of notes to be very comprehensive and nicely written.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/"&gt;This&lt;/a&gt; is another excellent reference with video lectures available on YouTube.
&lt;a href="https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/"&gt;This&lt;/a&gt; reference also covers a broad range of topic in depth.&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="detection theory"></category><category term="estimation theory"></category><category term="signal processing"></category></entry><entry><title>References for Statistical Signal Processing</title><link href="https://anmold-07.github.io/2020/01/01/my-super-post/" rel="alternate"></link><published>2020-01-01T10:25:00-05:00</published><updated>2020-05-01T19:30:00-04:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-01-01:/2020/01/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Statistical Signal Processing&lt;/p&gt;</summary><content type="html">&lt;p&gt;The topics covered in a course on Statistical Signal Processing and Estimation and Detection Theory are more or less similar. The following is a breakdown.
A course on estimation theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sufficient statistics&lt;/li&gt;
&lt;li&gt;Minimum variance unbiased estimation&lt;/li&gt;
&lt;li&gt;Cramer-Rao lower bound&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Bayesian estimation&lt;/li&gt;
&lt;li&gt;Wiener and Kalman filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A course on detection theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian risk theory&lt;/li&gt;
&lt;li&gt;Neyman-Pearson detection&lt;/li&gt;
&lt;li&gt;Signal detection in Gaussian noise&lt;/li&gt;
&lt;li&gt;Bayes factors and GLRTs&lt;/li&gt;
&lt;li&gt;CFAR detection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I found the following references helpful:&lt;/p&gt;
&lt;p&gt;Course notes for &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-432-stochastic-processes-detection-and-estimation-spring-2004/index.htm"&gt;this course&lt;/a&gt; was very helpful. 
To find the course notes refer to this &lt;a href="https://www.rle.mit.edu/sia/wp-content/uploads/2015/04/chapter1.pdf"&gt;link.&lt;/a&gt; and iterate through the chapters through links. I found these set of notes to be very comprehensive and nicely written.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/"&gt;This&lt;/a&gt; is another excellent reference with video lectures available on YouTube.
&lt;a href="https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/"&gt;This&lt;/a&gt; reference also covers a broad range of topic in depth.&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="detection theory"></category><category term="estimation theory"></category><category term="signal processing"></category></entry><entry><title>References for Statistical Signal Processing</title><link href="https://anmold-07.github.io/2020/01/01/my-super-post/" rel="alternate"></link><published>2020-01-01T10:25:00-05:00</published><updated>2020-05-01T19:30:00-04:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2020-01-01:/2020/01/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Statistical Signal Processing&lt;/p&gt;</summary><content type="html">&lt;p&gt;The topics covered in a course on Statistical Signal Processing and Estimation and Detection Theory are more or less similar. The following is a breakdown.
A course on estimation theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sufficient statistics&lt;/li&gt;
&lt;li&gt;Minimum variance unbiased estimation&lt;/li&gt;
&lt;li&gt;Cramer-Rao lower bound&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimation&lt;/li&gt;
&lt;li&gt;Bayesian estimation&lt;/li&gt;
&lt;li&gt;Wiener and Kalman filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A course on detection theory usually covers the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bayesian risk theory&lt;/li&gt;
&lt;li&gt;Neyman-Pearson detection&lt;/li&gt;
&lt;li&gt;Signal detection in Gaussian noise&lt;/li&gt;
&lt;li&gt;Bayes factors and GLRTs&lt;/li&gt;
&lt;li&gt;CFAR detection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I found the following references helpful:&lt;/p&gt;
&lt;p&gt;Course notes for &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-432-stochastic-processes-detection-and-estimation-spring-2004/index.htm"&gt;this course&lt;/a&gt; was very helpful. 
To find the course notes refer to this &lt;a href="https://www.rle.mit.edu/sia/wp-content/uploads/2015/04/chapter1.pdf"&gt;link.&lt;/a&gt; and iterate through the chapters through links. I found these set of notes to be very comprehensive and nicely written.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/"&gt;This&lt;/a&gt; is another excellent reference with video lectures available on YouTube.
&lt;a href="https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/"&gt;This&lt;/a&gt; reference also covers a broad range of topic in depth.&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="decision theory"></category><category term="detection theory"></category><category term="estimation theory"></category><category term="signal processing"></category></entry><entry><title>References for Probability Theory and Applications</title><link href="https://anmold-07.github.io/2019/09/01/my-super-post/" rel="alternate"></link><published>2019-09-01T10:25:00-04:00</published><updated>2019-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2019-09-01:/2019/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Stochastic Signals and Systems&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following references helped me understand important concepts in the field of, but not restricted to, probability theory, markov chains, time-series analysis etc. 
I hope these help you too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLbMVogVj5nJQqGHrpAloTec_lOKsG-foc"&gt;This playlist&lt;/a&gt; by Dr. Krishna Jagannathan from IIT Madras is an excellent reference for understanding the theory behind probability. This helped me become familiar with a lot sophisticated terms in the literature especially various concepts related to Borel-Cantelli lemmas and convergence of a sequence of random variables. These are quite abstract but explained beautifully in the lectures. Lecture Notes for this playlist can be found &lt;a href="http://www.ee.iitm.ac.in/~krishnaj/ee5110notes.htm"&gt;here.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="http://gtribello.github.io/mathNET/SOR3012.html"&gt;This course&lt;/a&gt; on Stochastic Processes is an excellent reference that focuses on both theoretical and practical aspects of advanced probability theory. It best explains continuous and discrete time markov chains with numerous practical examples.&lt;/p&gt;
&lt;p&gt;No reference is complete without MIT OCW's courses. &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"&gt;This course&lt;/a&gt; provides a comprehensive overview of discrete time Stochastic Processes with very comprehensive lecture notes. References &lt;a href="http://math.mit.edu/~sheffield/fall2019math600.html"&gt;here&lt;/a&gt; were also helpful.&lt;/p&gt;
&lt;p&gt;A comprehensive reference for practice problems on probability theory can be found &lt;a href="https://courses.grainger.illinois.edu/ece313/fa2019/old_exams.html"&gt;here.&lt;/a&gt;&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="stochastic signls"></category></entry><entry><title>References for Probability Theory and Applications</title><link href="https://anmold-07.github.io/2019/09/01/my-super-post/" rel="alternate"></link><published>2019-09-01T10:25:00-04:00</published><updated>2019-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2019-09-01:/2019/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Stochastic Signals and Systems&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following references helped me understand important concepts in the field of, but not restricted to, probability theory, markov chains, time-series analysis etc. 
I hope these help you too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLbMVogVj5nJQqGHrpAloTec_lOKsG-foc"&gt;This playlist&lt;/a&gt; by Dr. Krishna Jagannathan from IIT Madras is an excellent reference for understanding the theory behind probability. This helped me become familiar with a lot sophisticated terms in the literature especially various concepts related to Borel-Cantelli lemmas and convergence of a sequence of random variables. These are quite abstract but explained beautifully in the lectures. Lecture Notes for this playlist can be found &lt;a href="http://www.ee.iitm.ac.in/~krishnaj/ee5110notes.htm"&gt;here.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="http://gtribello.github.io/mathNET/SOR3012.html"&gt;This course&lt;/a&gt; on Stochastic Processes is an excellent reference that focuses on both theoretical and practical aspects of advanced probability theory. It best explains continuous and discrete time markov chains with numerous practical examples.&lt;/p&gt;
&lt;p&gt;No reference is complete without MIT OCW's courses. &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"&gt;This course&lt;/a&gt; provides a comprehensive overview of discrete time Stochastic Processes with very comprehensive lecture notes. References &lt;a href="http://math.mit.edu/~sheffield/fall2019math600.html"&gt;here&lt;/a&gt; were also helpful.&lt;/p&gt;
&lt;p&gt;A comprehensive reference for practice problems on probability theory can be found &lt;a href="https://courses.grainger.illinois.edu/ece313/fa2019/old_exams.html"&gt;here.&lt;/a&gt;&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="stochastic signls"></category></entry><entry><title>References for Probability Theory and Applications</title><link href="https://anmold-07.github.io/2019/09/01/my-super-post/" rel="alternate"></link><published>2019-09-01T10:25:00-04:00</published><updated>2019-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2019-09-01:/2019/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Stochastic Signals and Systems&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following references helped me understand important concepts in the field of, but not restricted to, probability theory, markov chains, time-series analysis etc. 
I hope these help you too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLbMVogVj5nJQqGHrpAloTec_lOKsG-foc"&gt;This playlist&lt;/a&gt; by Dr. Krishna Jagannathan from IIT Madras is an excellent reference for understanding the theory behind probability. This helped me become familiar with a lot sophisticated terms in the literature especially various concepts related to Borel-Cantelli lemmas and convergence of a sequence of random variables. These are quite abstract but explained beautifully in the lectures. Lecture Notes for this playlist can be found &lt;a href="http://www.ee.iitm.ac.in/~krishnaj/ee5110notes.htm"&gt;here.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="http://gtribello.github.io/mathNET/SOR3012.html"&gt;This course&lt;/a&gt; on Stochastic Processes is an excellent reference that focuses on both theoretical and practical aspects of advanced probability theory. It best explains continuous and discrete time markov chains with numerous practical examples.&lt;/p&gt;
&lt;p&gt;No reference is complete without MIT OCW's courses. &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"&gt;This course&lt;/a&gt; provides a comprehensive overview of discrete time Stochastic Processes with very comprehensive lecture notes. References &lt;a href="http://math.mit.edu/~sheffield/fall2019math600.html"&gt;here&lt;/a&gt; were also helpful.&lt;/p&gt;
&lt;p&gt;A comprehensive reference for practice problems on probability theory can be found &lt;a href="https://courses.grainger.illinois.edu/ece313/fa2019/old_exams.html"&gt;here.&lt;/a&gt;&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="stochastic signls"></category></entry><entry><title>References for Probability Theory and Applications</title><link href="https://anmold-07.github.io/2019/09/01/my-super-post/" rel="alternate"></link><published>2019-09-01T10:25:00-04:00</published><updated>2019-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2019-09-01:/2019/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Stochastic Signals and Systems&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following references helped me understand important concepts in the field of, but not restricted to, probability theory, markov chains, time-series analysis etc. 
I hope these help you too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLbMVogVj5nJQqGHrpAloTec_lOKsG-foc"&gt;This playlist&lt;/a&gt; by Dr. Krishna Jagannathan from IIT Madras is an excellent reference for understanding the theory behind probability. This helped me become familiar with a lot sophisticated terms in the literature especially various concepts related to Borel-Cantelli lemmas and convergence of a sequence of random variables. These are quite abstract but explained beautifully in the lectures. Lecture Notes for this playlist can be found &lt;a href="http://www.ee.iitm.ac.in/~krishnaj/ee5110notes.htm"&gt;here.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="http://gtribello.github.io/mathNET/SOR3012.html"&gt;This course&lt;/a&gt; on Stochastic Processes is an excellent reference that focuses on both theoretical and practical aspects of advanced probability theory. It best explains continuous and discrete time markov chains with numerous practical examples.&lt;/p&gt;
&lt;p&gt;No reference is complete without MIT OCW's courses. &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"&gt;This course&lt;/a&gt; provides a comprehensive overview of discrete time Stochastic Processes with very comprehensive lecture notes. References &lt;a href="http://math.mit.edu/~sheffield/fall2019math600.html"&gt;here&lt;/a&gt; were also helpful.&lt;/p&gt;
&lt;p&gt;A comprehensive reference for practice problems on probability theory can be found &lt;a href="https://courses.grainger.illinois.edu/ece313/fa2019/old_exams.html"&gt;here.&lt;/a&gt;&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="stochastic signls"></category></entry><entry><title>References for Probability Theory and Applications</title><link href="https://anmold-07.github.io/2019/09/01/my-super-post/" rel="alternate"></link><published>2019-09-01T10:25:00-04:00</published><updated>2019-12-01T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2019-09-01:/2019/09/01/my-super-post/</id><summary type="html">&lt;p&gt;References for a course on Stochastic Signals and Systems&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following references helped me understand important concepts in the field of, but not restricted to, probability theory, markov chains, time-series analysis etc. 
I hope these help you too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLbMVogVj5nJQqGHrpAloTec_lOKsG-foc"&gt;This playlist&lt;/a&gt; by Dr. Krishna Jagannathan from IIT Madras is an excellent reference for understanding the theory behind probability. This helped me become familiar with a lot sophisticated terms in the literature especially various concepts related to Borel-Cantelli lemmas and convergence of a sequence of random variables. These are quite abstract but explained beautifully in the lectures. Lecture Notes for this playlist can be found &lt;a href="http://www.ee.iitm.ac.in/~krishnaj/ee5110notes.htm"&gt;here.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="http://gtribello.github.io/mathNET/SOR3012.html"&gt;This course&lt;/a&gt; on Stochastic Processes is an excellent reference that focuses on both theoretical and practical aspects of advanced probability theory. It best explains continuous and discrete time markov chains with numerous practical examples.&lt;/p&gt;
&lt;p&gt;No reference is complete without MIT OCW's courses. &lt;a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"&gt;This course&lt;/a&gt; provides a comprehensive overview of discrete time Stochastic Processes with very comprehensive lecture notes. References &lt;a href="http://math.mit.edu/~sheffield/fall2019math600.html"&gt;here&lt;/a&gt; were also helpful.&lt;/p&gt;
&lt;p&gt;A comprehensive reference for practice problems on probability theory can be found &lt;a href="https://courses.grainger.illinois.edu/ece313/fa2019/old_exams.html"&gt;here.&lt;/a&gt;&lt;/p&gt;</content><category term="probability"></category><category term="randomness"></category><category term="stochastic signls"></category></entry><entry><title>Integrating Latex with Pelican</title><link href="https://anmold-07.github.io/2010/12/25/my-super-post/" rel="alternate"></link><published>2010-12-25T10:25:00-05:00</published><updated>2010-12-25T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-25:/2010/12/25/my-super-post/</id><summary type="html">&lt;p&gt;Pelican with latex&lt;/p&gt;</summary><content type="html">&lt;p&gt;Integrating latex in with Pelican is quite straight-forward. All you require is a Pelican plugin called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;render_math.&lt;/a&gt; Follow the link to set up latex with Pelican.&lt;/p&gt;</content><category term="math"></category><category term="latex"></category><category term="pelican"></category></entry><entry><title>Integrating Latex with Pelican</title><link href="https://anmold-07.github.io/2010/12/25/my-super-post/" rel="alternate"></link><published>2010-12-25T10:25:00-05:00</published><updated>2010-12-25T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-25:/2010/12/25/my-super-post/</id><summary type="html">&lt;p&gt;Pelican with latex&lt;/p&gt;</summary><content type="html">&lt;p&gt;Integrating latex in with Pelican is quite straight-forward. All you require is a Pelican plugin called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;render_math.&lt;/a&gt; Follow the link to set up latex with Pelican.&lt;/p&gt;</content><category term="math"></category><category term="latex"></category><category term="pelican"></category></entry><entry><title>Integrating Latex with Pelican</title><link href="https://anmold-07.github.io/2010/12/25/my-super-post/" rel="alternate"></link><published>2010-12-25T10:25:00-05:00</published><updated>2010-12-25T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-25:/2010/12/25/my-super-post/</id><summary type="html">&lt;p&gt;Pelican with latex&lt;/p&gt;</summary><content type="html">&lt;p&gt;Integrating latex in with Pelican is quite straight-forward. All you require is a Pelican plugin called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;render_math.&lt;/a&gt; Follow the link to set up latex with Pelican.&lt;/p&gt;</content><category term="math"></category><category term="latex"></category><category term="pelican"></category></entry><entry><title>Integrating Latex with Pelican</title><link href="https://anmold-07.github.io/2010/12/25/my-super-post/" rel="alternate"></link><published>2010-12-25T10:25:00-05:00</published><updated>2010-12-25T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-25:/2010/12/25/my-super-post/</id><summary type="html">&lt;p&gt;Pelican with latex&lt;/p&gt;</summary><content type="html">&lt;p&gt;Integrating latex in with Pelican is quite straight-forward. All you require is a Pelican plugin called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;render_math.&lt;/a&gt; Follow the link to set up latex with Pelican.&lt;/p&gt;</content><category term="math"></category><category term="latex"></category><category term="pelican"></category></entry><entry><title>Integrating Latex with Pelican</title><link href="https://anmold-07.github.io/2010/12/25/my-super-post/" rel="alternate"></link><published>2010-12-25T10:25:00-05:00</published><updated>2010-12-25T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-25:/2010/12/25/my-super-post/</id><summary type="html">&lt;p&gt;Pelican with latex&lt;/p&gt;</summary><content type="html">&lt;p&gt;Integrating latex in with Pelican is quite straight-forward. All you require is a Pelican plugin called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;render_math.&lt;/a&gt; Follow the link to set up latex with Pelican.&lt;/p&gt;</content><category term="math"></category><category term="latex"></category><category term="pelican"></category></entry><entry><title>Setting up this website</title><link href="https://anmold-07.github.io/2010/12/03/my-super-post/" rel="alternate"></link><published>2010-12-03T10:25:00-05:00</published><updated>2010-12-05T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-03:/2010/12/03/my-super-post/</id><summary type="html">&lt;p&gt;Setting up an Static Site Generator (SSG)&lt;/p&gt;</summary><content type="html">
&lt;h2 id="references-for-setting-up-a-ssg"&gt;References for setting up a SSG&lt;/h2&gt;
&lt;p&gt;This blog was setup with the help of this well documented blog on setting up a &lt;a href="https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"&gt;Static Site Generator&lt;/a&gt;. Highly encourage you to go through it in case you require to set up your own static website.&lt;/p&gt;</content><category term="pelican"></category><category term="publishing"></category><category term="blog"></category></entry><entry><title>Setting up this website</title><link href="https://anmold-07.github.io/2010/12/03/my-super-post/" rel="alternate"></link><published>2010-12-03T10:25:00-05:00</published><updated>2010-12-05T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-03:/2010/12/03/my-super-post/</id><summary type="html">&lt;p&gt;Setting up an Static Site Generator (SSG)&lt;/p&gt;</summary><content type="html">
&lt;h2 id="references-for-setting-up-a-ssg"&gt;References for setting up a SSG&lt;/h2&gt;
&lt;p&gt;This blog was setup with the help of this well documented blog on setting up a &lt;a href="https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"&gt;Static Site Generator&lt;/a&gt;. Highly encourage you to go through it in case you require to set up your own static website.&lt;/p&gt;</content><category term="pelican"></category><category term="publishing"></category><category term="blog"></category></entry><entry><title>Setting up this website</title><link href="https://anmold-07.github.io/2010/12/03/my-super-post/" rel="alternate"></link><published>2010-12-03T10:25:00-05:00</published><updated>2010-12-05T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-03:/2010/12/03/my-super-post/</id><summary type="html">&lt;p&gt;Setting up an Static Site Generator (SSG)&lt;/p&gt;</summary><content type="html">
&lt;h2 id="references-for-setting-up-a-ssg"&gt;References for setting up a SSG&lt;/h2&gt;
&lt;p&gt;This blog was setup with the help of this well documented blog on setting up a &lt;a href="https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"&gt;Static Site Generator&lt;/a&gt;. Highly encourage you to go through it in case you require to set up your own static website.&lt;/p&gt;</content><category term="pelican"></category><category term="publishing"></category><category term="blog"></category></entry><entry><title>Setting up this website</title><link href="https://anmold-07.github.io/2010/12/03/my-super-post/" rel="alternate"></link><published>2010-12-03T10:25:00-05:00</published><updated>2010-12-05T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-03:/2010/12/03/my-super-post/</id><summary type="html">&lt;p&gt;Setting up an Static Site Generator (SSG)&lt;/p&gt;</summary><content type="html">
&lt;h2 id="references-for-setting-up-a-ssg"&gt;References for setting up a SSG&lt;/h2&gt;
&lt;p&gt;This blog was setup with the help of this well documented blog on setting up a &lt;a href="https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"&gt;Static Site Generator&lt;/a&gt;. Highly encourage you to go through it in case you require to set up your own static website.&lt;/p&gt;</content><category term="pelican"></category><category term="publishing"></category><category term="blog"></category></entry><entry><title>Setting up this website</title><link href="https://anmold-07.github.io/2010/12/03/my-super-post/" rel="alternate"></link><published>2010-12-03T10:25:00-05:00</published><updated>2010-12-05T19:30:00-05:00</updated><author><name>Anmol</name></author><id>tag:anmold-07.github.io,2010-12-03:/2010/12/03/my-super-post/</id><summary type="html">&lt;p&gt;Setting up an Static Site Generator (SSG)&lt;/p&gt;</summary><content type="html">
&lt;h2 id="references-for-setting-up-a-ssg"&gt;References for setting up a SSG&lt;/h2&gt;
&lt;p&gt;This blog was setup with the help of this well documented blog on setting up a &lt;a href="https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"&gt;Static Site Generator&lt;/a&gt;. Highly encourage you to go through it in case you require to set up your own static website.&lt;/p&gt;</content><category term="pelican"></category><category term="publishing"></category><category term="blog"></category></entry></feed>